## 前沿 ##

[笔记WiKi](https://github.com/476139183/Structure-and-Algorithm/wiki)

#### 为什么我要学算法 ####

2016年的时候，出去面试，无论是大公司 还是创业的小公司，都会问及到算法，甚至是手写算法。当时就纳闷了，首先在工作中真真不需要使用到算法，而且我还只是一个初级程序员，我感觉我连自己的编程语言都不太熟悉，更别说妄论算法了。但为了应付面试，只能去了解算法，买了一本《算法导论》看，果不其然，理解很困难，学的很累，关键是还没学懂，无奈放弃。

时间如梭，转眼2019，偶然看到王争老师的[《数据结构与算法之美》](https://time.geekbang.org/column/article/39922),老师的豪言壮语，以及入门几篇的文章解读，学习算法之心逐渐涌现，也许是为了以后的面试，也许是为了扩展视野，我总算入手了这门课程。希望能真正的如王争老师所说，能迈过 **数据结构和算法分析** 这道坎。

#### 人们无法理解他没有经历过的事物 ####

最初的一个多月的学习，真的不太顺利，很多东西没有吃透，没理解，专栏看的断断续续，陷入一个小困境，需要反复咀嚼篇文，但依然忘这忘那。不过所幸专栏已经让我看到了 算法的 门槛，学起来有目标。我后面就开始换一个学习方式，不再强求每一篇文章要求自己熟练，而是看一遍，尽量的看到最新的篇章之后，再回头看一遍，再攻读难点篇章。整体的阅读思考，局部的咀嚼，让自己形成有一个小小的算法思想。果然如老师所说 **"掌握了数据结构与算法，看待问题的深度，解决问题的角度就会完全不一样了"**    

<<<<<<< HEAD
这个专栏，我不要你觉得，我要我觉得，老师讲的的确好，是真真的带我入了算法门槛，所以
后面附有邀请码，扫码购买更优惠哦（手动滑稽）


## 算法的由来 ##

> 最早出现在波斯数学家阿勒.花刺子密在公元825年所写的《印度数字算术》中。

#### 算法的定义       

 是解决特定问题求解步骤的描述，在计算机中表现为指令的有限序列，并且每一条指令表示一个或多个操作。 

#### 算法有5大基本特性

`输入`,`输出`,`有穷性`,`确定性`,`可行性`

##### 1.输入和输出 

算法具有0个或者多个输入，比如你输出 "Hello world",那就不需要输入，但是至少有一个输出，毕竟你不要输出，你用这个算法干嘛？ 

##### 2. 有穷性

指算法在执行有限步骤之后，自动结束而不会出现无限循环，并且每一个步骤在可接受的时间内完成。这个时间要实际合理，而不是说 需要20多年才能结束。

##### 3. 确定性

算法的每一步骤都具有确定的含义，不会出现二义性，算法的每个步骤被精确定义而无歧议

##### 4. 可行性

算法的每一步都必须是可行的，每一步都能通过执行有限次数完成，这意味着算法可以转换为程序在机器上运行并得到正确的结果。

#### 算法的设计要求

`正确性`，`可读性`，`健壮性`，`时间效率高和存储量低`

## 大O复杂度表示法 ##

> 这是计算机前辈们，为了对算法的评估更科学，而研究出的事前分析估算法。也就是在计算机程序编制前，依据统计方法 用 “肉眼” 对算法的执行时间进行粗略估算。

一个程序在计算机运行的时间取决于下面几个因素 

1.`算法的策略和方法`      
2.`编译产生的代码质量`     
3. `问题的输入规模`      
4.`机器执行指令的速度`     

抛开软件与硬件相关的因素，我们发现，输入规模能够影响到算法的时间，如下代码


```
 int cal(int n) {
   int sum = 0;
   int i = 1;
   for (; i <= n; ++i) {
     sum = sum + i;
   }
   return sum;
 }

```

我们假设每一行代码 需要 1个 unit_time的执行时间，第4，5行执行了n遍，那么这段代码的总执行时间公式如下：      

T(n)=(2n+2)*unit_time

我们再看一段代码

```
 int cal(int n) {
   int sum = 0;
   int i = 1;
   int j = 1;
   for (; i <= n; ++i) {
     j = 1;
     for (; j <= n; ++j) {
       sum = sum +  i * j;
     }
   }
 }

```

我们同样去分析： 第 2、3、4 行代码，每行都需要 1 个 unit_time 的执行时间，第 5、6 行代码循环执行了 n 遍，需要 2n * unit_time 的执行时间，第 7、8 行代码循环执行了 n2遍，所以需要 2n2 * unit_time 的执行时间。所以，整段代码总的执行时间 T(n) = (2n2+2n+3)*unit_time。


我们可以看到，代码总执行的时间 T(n) 与 输入的 n 有关系，确切的来说 那就是 
<font color="darkOrange">所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比</font>

![趋势图](https://github.com/476139183/Structure-and-Algorithm/raw/master/Images/趋势图.png)


于是我们总结出一个公式:

![](https://github.com/476139183/Structure-and-Algorithm/raw/master/Images/大O表示法.png)

这就是 大O时间复杂度表示法，它代表着 代码执行的时间随着数据规模增长的变化趋势，也称之为“渐进时间复杂度”

在 大O时间复杂度表示法 中，我们只关注 程序执行时间与 数据规模之间的关系，而忽略一些低阶、常量、系数 。而且只记录一个最大量级的部分。

也就是前面两段代码的时间复杂度，记为：T(n) = O(n)； T(n) = O(n2)。


## 推导 大O 时间复杂度 ##

前面已经有介绍 大O 时间复杂度的由来和表示，下面开始进行分析和推导

1. 只关注循环执行次数最到的一段代码
   大 O 这种复杂度表示方法 只是表示与 数据规模 n 的变化趋势，所以只记录最大阶的量级，并且要忽略公式中的 常量、低阶、系数，所以 **我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的哪一段代码就可以了** 

2. 加法法则：总复杂度等于量级最大的那段代码的复杂度

3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

这么来看，还是不够清晰，那么再看看《大话数据结构》给我们的攻略吧

1. 用常数1取代运行时间中所有的加法常数
2. 在修改后的运行次数函数中，只保留最高阶项
3. 如果最高阶项存在且不是1，则去除与这个项相乘的常数

这里有一个常见的时间复杂度表，方便我们自己推导和理解

![复杂度表](https://github.com/476139183/Structure-and-Algorithm/raw/master/Images/复杂度表.png)

> 多去分析,和运用，就能很快的理解这个推导法了


## 后记 ##

最淡的墨水也胜于最强的记忆。学完之后，也总结一下，加深自己的印象和理解。

推荐两个顶尖学校的算法课程：

第一个是[MIT的算法导论公开课](http://open.163.com/special/opencourse/algorithms.html)，网易公开课有中文字幕。

第二个是[普林斯顿大学的算法课程](https://www.bilibili.com/video/av8994940)，可以在B站免费观看

当然还有我的邀请码，毕竟笔记肯定没有老师讲的好

![邀请码](https://github.com/476139183/Structure-and-Algorithm/raw/master/Images/WechatCode.png)
=======
我的[文章链接](https://476139183.github.io)

![11111](/Images/WechatCode.png)
>>>>>>> ae2c4f4e59fda3e7c1c4c3cb491375bd5092af5c
